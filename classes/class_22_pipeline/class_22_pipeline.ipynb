{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bc48b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/Machine-Learning/blob/main/ML/classes/class_march_3/class_march_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85bc27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clase 22 Pipeline (4 de marzo de 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f541c49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objetivo  \n",
    "\n",
    "4. Preprocesamiento usando el concepto de pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04893f61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Librerías necesarias para la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8d90e20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e265465c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class AdAtribComb(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ad_dph = True, ad_hph = True, ad_pph = True): # no *args or **kargs\n",
    "        self.ad_dph = ad_dph\n",
    "        self.ad_hph = ad_hph\n",
    "        self.ad_pph = ad_pph\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        habitaciones, dormitorios, población, hogares = 3, 4, 5, 6 \n",
    "        if self.ad_dph:\n",
    "            dormitorios_por_habitación=X[:,dormitorios]/X[:,habitaciones]\n",
    "            X=np.c_[X, dormitorios_por_habitación]\n",
    "        if self.ad_hph:\n",
    "            habitaciones_por_hogar=X[:,habitaciones]/X[:,hogares]\n",
    "            X=np.c_[X, habitaciones_por_hogar]\n",
    "        if self.ad_pph: \n",
    "            población_por_hogar = X[:, población]/X[:, hogares]\n",
    "            X=np.c_[X, población_por_hogar] \n",
    "        return X\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72634293",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obtención de los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1027b28a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "v = pd.read_csv('vivienda.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0509c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## División en entrenamiento y testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7842e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "v_train,v_test = train_test_split(v,test_size = 0.2, random_state = 513) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc8e2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## División en predictores y objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba861291",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "v = v_train.drop('precio', axis = 1)\n",
    "v_labels = v_train.precio.values.ravel() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b37dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## División en predictores numéricos y predictores categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d10e5a8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "v_num = v.drop('proximidad', axis = 1)\n",
    "v_cat = v.proximidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c86a65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformation Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761dca8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como puede ver, hay muchos pasos de transformación de datos que deben ejecutarse en el orden correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f17ecd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Afortunadamente, Scikit-Learn proporciona la clase Pipeline para ayudar con tales secuencias de transformaciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff2163e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aquí hay una pequeña tubería para los atributos numéricos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6c711dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_num = Pipeline([\n",
    "                        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "                        ('attribs_adder', AdAtribComb()),\n",
    "                        ('std_scaler', StandardScaler()),\n",
    "])\n",
    "x_num_tr = pipeline_num.fit_transform(v_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a79c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El constructor Pipeline toma una lista de pares de nombre/estimador que definen una secuencia de pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bab5b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All but the last estimator must be transformers (i.e., they must have a fit_transform() method). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde11f6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The names can be anything you like\n",
    "(as long as they are unique and don’t contain double underscores, __); they will\n",
    "come in handy later for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9cfcaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When you call the pipeline’s `fit()` method, it calls `fit_transform()` sequentially on all transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it calls the `fit()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e11ac3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The pipeline exposes the same methods as the final estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcbc4cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this example, the last estimator is a StandardScaler, which is a transformer, so the pipeline has a `transform()` method that applies all the transforms to the data in sequence (and of course also a `fit_transform()` method, which is the one we used)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff41ed9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So far, we have handled the categorical columns and the numerical columns separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183737f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e54577",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In version 0.20, Scikit-Learn introduced the ColumnTransformer for this purpose, and the good news is that it works great with pandas DataFrames. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ffe09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s use it to apply all the transformations to the housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac038e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "lista_atributos_num = list(v_num)\n",
    "lista_atributos_cat = [\"proximidad\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "                                   (\"num\", pipeline_num, lista_atributos_num),\n",
    "                                   (\"cat\", OneHotEncoder(), lista_atributos_cat),\n",
    "                                    ])\n",
    "\n",
    "X_prep = full_pipeline.fit_transform(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe95ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First we import the ColumnTransformer class, next we get the list of numerical column names and the list of categorical column names, and then we construct a ColumnTransformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92f51d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The constructor requires a list of tuples, where each\n",
    "tuple contains a name, a transformer, and a list of names (or indices) of\n",
    "columns that the transformer should be applied to. In this example, we specify\n",
    "that the numerical columns should be transformed using the num_pipeline that\n",
    "we defined earlier, and the categorical columns should be transformed using a\n",
    "OneHotEncoder. Finally, we apply this ColumnTransformer to the housing\n",
    "data: it applies each transformer to the appropriate columns and concatenates the outputs along the second axis (the transformers must return the same\n",
    "number of rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c68959",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the OneHotEncoder returns a sparse matrix, while the num_pipeline\n",
    "returns a dense matrix. When there is such a mix of sparse and dense matrices,\n",
    "the ColumnTransformer estimates the density of the final matrix (i.e., the ratio\n",
    "of nonzero cells), and it returns a sparse matrix if the density is lower than a\n",
    "given threshold (by default, sparse_threshold=0.3). In this example, it\n",
    "returns a dense matrix. And that’s it! We have a preprocessing pipeline that\n",
    "takes the full housing data and applies the appropriate transformations to each\n",
    "column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4caa28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee2ba2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead of using a transformer, you can specify the string \"drop\" if you want the columns to\n",
    "be dropped, or you can specify \"passthrough\" if you want the columns to be left untouched.\n",
    "By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but\n",
    "you can set the remainder hyperparameter to any transformer (or to \"passthrough\") if you\n",
    "want these columns to be handled differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75f514",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as sklearn-pandas, or you can roll out your own custom transformer to get the same functionality as the ColumnTransformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac9c12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alternatively, you can use the FeatureUnion class, which can apply different transformers and\n",
    "concatenate their outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471eb9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But you cannot specify different columns for each transformer; they all apply to the whole data. It is possible to work around this limitation using a custom transformer for column selection (see the Jupyter notebook for an example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250733cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Select and Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f91df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At last! You framed the problem, you got the data and explored it, you sampled a training set and a test set, and you wrote transformation pipelines to clean up and prepare your data for Machine Learning algorithms automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c057291",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You are now ready to select and train a Machine Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586fb19b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
