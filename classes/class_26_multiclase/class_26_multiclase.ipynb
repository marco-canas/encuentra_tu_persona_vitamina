{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b594809d",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/intro-Machine-Learning/blob/main/classes/class_25_desempa%C3%B1o_clasificador/class_25_medidas_desempeno_clasificador.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21875ff",
   "metadata": {},
   "source": [
    "# Medidas de desempeño de un Clasificador y clasificación multiclase: Clase 26 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f955712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.datasets import fetch_openml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d15e9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "mnist = fetch_openml('mnist_784', version = 1, as_frame = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "394905a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = mnist['data'], mnist['target'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44bb564",
   "metadata": {},
   "source": [
    "# Conversión de `str` a `int64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72e3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60_000], X[60_000:], y[:60_000], y[60_000:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train==5)\n",
    "y_test_5 = (y_test==5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96cc8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfcb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "sgd_clf.fit(X_train, y_train_5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db09de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.predict(X_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58899966",
   "metadata": {},
   "source": [
    "# La validación cruzada con la medida de desempeño de la exactitud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8f6c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train_5[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train_5[test_index]\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016189c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c56acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "puntajes = cross_val_score(sgd_clf, X_train, y_train_5, cv = 10, scoring = 'accuracy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16edc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "puntajes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edad088",
   "metadata": {},
   "source": [
    "Nota:  \n",
    "\n",
    "`shuffle=True` se omitió por error en versiones anteriores del libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402bce17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037eb084",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a43e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198d48b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " precision_score(y_train_5, y_predict2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_predict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8999466",
   "metadata": {},
   "source": [
    "Now your 5-detector does not look as shiny as it did when you looked at its accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404ebaa",
   "metadata": {},
   "source": [
    "When it claims an image represents a 5, it is correct only 72.9% of the time. Moreover, it only detects 75.6% of the 5s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a079c7",
   "metadata": {},
   "source": [
    "It is often convenient to combine precision and recall into a single metric called the $F_{1}$ score, in particular if you need a simple way to compare two classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c83181",
   "metadata": {},
   "source": [
    "The $F_{1}$ score is the harmonic mean of precision and recall (Equation 3-3). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552436d",
   "metadata": {},
   "source": [
    "Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60baec",
   "metadata": {},
   "source": [
    "As a result, the classifier will only get a high $F_{1}$ score if both recall and precision are high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e8258f",
   "metadata": {},
   "source": [
    "$$ F_{1} = \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}} } = 2 \\cdot \\frac{}{} =   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446d9e9",
   "metadata": {},
   "source": [
    "To compute the $F_{1}$ score, simply call the `f1_score()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301bfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82d22b",
   "metadata": {},
   "source": [
    "The F score favors classifiers that have similar precision and recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff02890",
   "metadata": {},
   "source": [
    "This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002e88c",
   "metadata": {},
   "source": [
    "For example, if you trained a classifier to detect videos that are safe for kids, you would\n",
    "probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a classifier that has a much higher recall but lets a few really bad videos show up in your product (in such cases, you may even want to add a human pipeline to check the classifier’s video selection). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d803f1",
   "metadata": {},
   "source": [
    "On the other hand, suppose you train a classifier to detect shoplifters in surveillance images: it is probably fine if your classifier has only 30% precision as long as it has 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get\n",
    "caught)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770cce61",
   "metadata": {},
   "source": [
    "Unfortunately, you can’t have it both ways: increasing precision reduces\n",
    "recall, and vice versa. This is called the precision/recall trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abfce10",
   "metadata": {},
   "source": [
    "## Precision/Recall Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a60dac3",
   "metadata": {},
   "source": [
    "To understand this trade-off, let’s look at how the SGDClassifier makes its classification decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2d30f",
   "metadata": {},
   "source": [
    "For each instance, it computes a score based on a decision function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee98cc",
   "metadata": {},
   "source": [
    "If that score is greater than a threshold, it assigns the instance to the positive class; otherwise it assigns it to the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab394a",
   "metadata": {},
   "source": [
    "Figure 3-3 shows a few digits positioned from the lowest score on the left to the highest score on the right. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9005104",
   "metadata": {},
   "source": [
    "Suppose the decision threshold is positioned at the central arrow (between the two 5s): you will find 4 true positives (actual 5s) on the right of that threshold, and 1 false positive (actually a 6). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbdfe0f",
   "metadata": {},
   "source": [
    "Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfbf76",
   "metadata": {},
   "source": [
    "If you raise the threshold (move it to the arrow on the right), the false positive (the 6) becomes a true negative, thereby increasing the precision (up to 100% in this case), but one true positive becomes a false negative, decreasing recall down to 50%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b1348",
   "metadata": {},
   "source": [
    "Conversely, lowering the threshold increases recall and reduces precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd681185",
   "metadata": {},
   "source": [
    "<img src = 'https://github.com/marco-canas/intro-Machine-Learning/blob/main/classes/class_26_multiclase/figura_3_3_various_thresholds.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7c9cb",
   "metadata": {},
   "source": [
    "Scikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores that it uses to make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f939bc4",
   "metadata": {},
   "source": [
    "Instead of calling the classifier’s `predict()` method, you can call its `decision_function()` method, which returns a score for each instance, and then use any threshold you want to make predictions based on those scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cb807",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = sgd_clf.decision_function([X[0]])\n",
    "y_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41166cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3a25b",
   "metadata": {},
   "source": [
    "The SGDClassifier uses a threshold equal to 0, so the previous code returns the same result as the predict() method (i.e., True). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb98e469",
   "metadata": {},
   "source": [
    "Let’s raise the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b4709",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> threshold = 8000\n",
    ">>> y_some_digit_pred = (y_scores > threshold)\n",
    ">>> y_some_digit_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ecd332",
   "metadata": {},
   "source": [
    "This confirms that raising the threshold decreases recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb593ac",
   "metadata": {},
   "source": [
    "The image actually represents a 5, and the classifier detects it when the threshold is 0, but it misses it when the threshold is increased to 8,000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0093ae",
   "metadata": {},
   "source": [
    "How do you decide which threshold to use? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35422023",
   "metadata": {},
   "source": [
    "First, use the cross_val_predict() function to get the scores of all instances in the training set, but this time specify that you want to return decision scores instead of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6cbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed2d3be",
   "metadata": {},
   "source": [
    "With these scores, use the precision_recall_curve() function to compute precision and recall for all possible thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ceadb",
   "metadata": {},
   "source": [
    "Finally, use Matplotlib to plot precision and recall as functions of the threshold value (Figure 3-4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6460676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    [...] # highlight the threshold and add the legend, axis label, and grid\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b548f",
   "metadata": {},
   "source": [
    "<img src = 'https://github.com/marco-canas/intro-Machine-Learning/blob/main/classes/class_26_multiclase/figura_3_4_thresholds.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8eb08",
   "metadata": {},
   "source": [
    "## NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b3895",
   "metadata": {},
   "source": [
    "You may wonder why the precision curve is bumpier than the recall curve in Figure 3-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd6410",
   "metadata": {},
   "source": [
    "The reason is that precision may sometimes go down when you raise the threshold (although in general it will go up). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6d38f",
   "metadata": {},
   "source": [
    "To understand why, look back at Figure 3-3 and notice what happens when you start from the central threshold and move it just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a5e0d2",
   "metadata": {},
   "source": [
    "On the other hand, recall can only go down when the threshold is increased, which explains why its curve looks smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81da8749",
   "metadata": {},
   "source": [
    "Another way to select a good precision/recall trade-off is to plot precision directly against recall, as shown in Figure 3-5 (the same threshold as earlier is highlighted)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8906a5",
   "metadata": {},
   "source": [
    "<img src = 'https://github.com/marco-canas/intro-Machine-Learning/blob/main/classes/class_26_multiclase/figura_3_5_recall_precision.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7160318d",
   "metadata": {},
   "source": [
    "You can see that precision really starts to fall sharply around 80% recall.\n",
    "You will probably want to select a precision/recall trade-off just before that\n",
    "drop—for example, at around 60% recall. But of course, the choice depends\n",
    "on your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0a962",
   "metadata": {},
   "source": [
    "Suppose you decide to aim for 90% precision. You look up the first plot and\n",
    "find that you need to use a threshold of about 8,000. To be more precise you\n",
    "can search for the lowest threshold that gives you at least 90% precision\n",
    "(np.argmax() will give you the first index of the maximum value, which in\n",
    "this case means the first True value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74450a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)] # ~7816"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518783c4",
   "metadata": {},
   "source": [
    "To make predictions (on the training set for now), instead of calling the\n",
    "classifier’s predict() method, you can run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores >= threshold_90_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd236ed",
   "metadata": {},
   "source": [
    "Let’s check these predictions’ precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afcdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> precision_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> recall_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6ebdb",
   "metadata": {},
   "source": [
    "Great, you have a 90% precision classifier! As you can see, it is fairly easy\n",
    "to create a classifier with virtually any precision you want: just set a high\n",
    "enough threshold, and you’re done. But wait, not so fast. A high-precision\n",
    "classifier is not very useful if its recall is too low!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43152626",
   "metadata": {},
   "source": [
    "## TIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e97dc6",
   "metadata": {},
   "source": [
    "If someone says, “Let’s reach 99% precision,” you should ask, “At what recall?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb556c",
   "metadata": {},
   "source": [
    "## The ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2f34f",
   "metadata": {},
   "source": [
    "The receiver operating characteristic (ROC) curve is another common tool\n",
    "used with binary classifiers. It is very similar to the precision/recall curve,\n",
    "but instead of plotting precision versus recall, the ROC curve plots the true\n",
    "positive rate (another name for recall) against the false positive rate (FPR).\n",
    "The FPR is the ratio of negative instances that are incorrectly classified as\n",
    "positive. It is equal to 1 – the true negative rate (TNR), which is the ratio of\n",
    "negative instances that are correctly classified as negative. The TNR is also\n",
    "called specificity. Hence, the ROC curve plots sensitivity (recall) versus 1 –\n",
    "specificity.\n",
    "To plot the ROC curve, you first use the roc_curve() function to compute\n",
    "the TPR and FPR for various threshold values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0ac97",
   "metadata": {},
   "source": [
    "Then you can plot the FPR against the TPR using Matplotlib. This code\n",
    "produces the plot in Figure 3-6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771cf068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal\n",
    "    [...] # Add axis labels and grid\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acef9d9",
   "metadata": {},
   "source": [
    "Once again there is a trade-off: the higher the recall (TPR), the more false positives (FPR) the classifier produces. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d17978",
   "metadata": {},
   "source": [
    "The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80bf135",
   "metadata": {},
   "source": [
    "<img src = 'https://github.com/marco-canas/intro-Machine-Learning/blob/main/classes/class_26_multiclase/figura_3_6_false_positive_rate_vs_true_positive_rate.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79fb77",
   "metadata": {},
   "source": [
    "One way to compare classifiers is to measure the area under the curve (AUC). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa5311",
   "metadata": {},
   "source": [
    "A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc36f4",
   "metadata": {},
   "source": [
    "## TIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc162aa2",
   "metadata": {},
   "source": [
    "Since the ROC curve is so similar to the precision/recall (PR) curve, you may wonder\n",
    "how to decide which one to use. As a rule of thumb, you should prefer the PR curve\n",
    "whenever the positive class is rare or when you care more about the false positives than\n",
    "the false negatives. Otherwise, use the ROC curve. For example, looking at the previous\n",
    "ROC curve (and the ROC AUC score), you may think that the classifier is really good.\n",
    "But this is mostly because there are few positives (5s) compared to the negatives (non-\n",
    "5s). In contrast, the PR curve makes it clear that the classifier has room for improvement\n",
    "(the curve could be closer to the top-left corner).\n",
    "Let’s now train a RandomForestClassifier and compare its ROC curve\n",
    "and ROC AUC score to those of the SGDClassifier. First, you need to get\n",
    "scores for each instance in the training set. But due to the way it works (see\n",
    "Chapter 7), the RandomForestClassifier class does not have a\n",
    "decision_function() method. Instead, it has a predict_proba()\n",
    "method. Scikit-Learn classifiers generally have one or the other, or both.\n",
    "The predict_proba() method returns an array containing a row per\n",
    "instance and a column per class, each containing the probability that the\n",
    "given instance belongs to the given class (e.g., 70% chance that the image\n",
    "represents a 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789388c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
    "                                     method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d2b2dd",
   "metadata": {},
   "source": [
    "The roc_curve() function expects labels and scores, but instead of scores you can give it class probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877937fd",
   "metadata": {},
   "source": [
    "Let’s use the positive class’s probability as the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
    "fpr_forest, tpr_forest, thresholds_forest =\n",
    "roc_curve(y_train_5,y_scores_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308e75f8",
   "metadata": {},
   "source": [
    "Now you are ready to plot the ROC curve. It is useful to plot the first ROC curve as well to see how they compare (Figure 3-7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d591759",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f2929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a338ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a9dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f54d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f6a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd25562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ddc256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7841a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e6353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575cc00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8f295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73ea2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5095cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d26f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ef8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
