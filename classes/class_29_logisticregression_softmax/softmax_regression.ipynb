{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8821f879",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/intro-Machine-Learning/blob/main/classes/class_26_multiclase/class_26_multiclase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8598f03b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c88cb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El modelo de Regresión Logística se puede generalizar para soportar múltiples clases directamente, sin tener que entrenar y combinar múltiples clasificadores binarios (como se discutió en el Capítulo 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb001157",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama regresión Softmax o regresión logística multinomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f648995",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The idea is simple: when given an instance x, the Softmax Regression model first computes a score $s(x)$ for each class $k$, then estimates the probability of each class by applying the softmax function (also called the normalized exponential) to the scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b3b71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The equation to compute $s(x)$ should look familiar, as it is just like the equation for Linear Regression prediction (see Equation 4- 19)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387fb5a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-19. Softmax score for class k\n",
    "\n",
    "$$ S_{k}(x) = x^{T}\\theta^{(K)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b3914",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that each class has its own dedicated parameter vector $\\theta^{(k)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bbc5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All these vectors are typically stored as rows in a *parameter matrix* $\\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849a3e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once you have computed the score of every class for the instance $x$, you can estimate the probability $\\hat{p}$ that the instance belongs to class $k$ by running the scores through the softmax function (Equation 4-20). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca8b64",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The function computes the exponential of every score, then normalizes them (dividing by the sum of all the exponentials). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5124cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The scores are generally called logits or log-odds (although they are actually unnormalized log-odds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e092e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-20. Softmax function  \n",
    "\n",
    "$$ \\hat{p}_{k} = \\sigma(s(x))_{k} = \\frac{\\exp(s_{k}(x))}{\\sigma_{j = 1}^{k}\\exp(s_{j}(x))}  $$\n",
    "\n",
    "In this equation:  \n",
    "* $K$ is the number of classes.  \n",
    "* $s(x)$ is a vector containing the scores of each class for the instance $x$.  \n",
    "* $\\sigma(s(x))$ is the estimated probability that the instance $x$ belongs to class $k$, given the scores of each class for that instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f8954",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b46fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00daf291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d829b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00ee73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e893e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc089f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
